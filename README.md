# DeepLearning_Architecture



## 1. Transformer:Attention Is All You Need (1주차)

논문
- https://arxiv.org/abs/1706.03762

참고 블로그 
- https://paul-hyun.github.io/transformer-01/ (논문구현1 pytorch)
- https://cpm0722.github.io/pytorch-implementation/transformer (논문구현2 pytorch)


논문 참고 유튜브
- https://www.youtube.com/watch?v=AA621UofTUA&t=5s (동빈나)
- https://www.youtube.com/watch?v=x_8cp4Vdnak (DSBA 연구실)

참고 깃허브
- https://github.com/huggingface/transformers

## 2. GPT (2주차)

논문 
- https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

참고 블로그
- https://lsjsj92.tistory.com/617 (논문리뷰)
- https://dsbook.tistory.com/321 (논문리뷰)

## 3. BERT (3주차)

논문
- https://arxiv.org/abs/1810.04805

참고 블로그
- http://yonghee.io/bert_binary_classification_naver/ (BERT로 영화리뷰데이터 분류)
- https://ebbnflow.tistory.com/163?category=895676 (캐글문제 분류)
